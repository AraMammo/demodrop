### Complete CheerioCrawler Setup and Execution Example

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This comprehensive example combines all previous steps to demonstrate a fully functional `CheerioCrawler`. It sets up the `RequestQueue`, defines the `handlePageFunction`, configures the `CheerioCrawler` with these components, and then runs the crawler to process requests and extract data.

```javascript
const Apify = require('apify');

Apify.main(async () => {
    const requestQueue = await Apify.openRequestQueue();
    await requestQueue.addRequest({ url: 'https://apify.com' });

    const handlePageFunction = async ({ request, $ }) => {
        const title = $('title').text();

        console.log(`The title of "${request.url}" is: ${title}.`);
    };

    // Set up the crawler, passing a single options object as an argument.
    const crawler = new Apify.CheerioCrawler({
        requestQueue,
        handlePageFunction,
    });

    await crawler.run();
});
```

--------------------------------

### Basic Apify CheerioCrawler Implementation

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This example demonstrates a fundamental setup for Apify's CheerioCrawler. It initializes a RequestQueue with a starting URL, defines a `handlePageFunction` to process the downloaded HTML using Cheerio (extracting the page title), and then runs the crawler. This showcases the core components required for a simple CheerioCrawler operation.

```JavaScript
const Apify = require('apify');

Apify.main(async () => {
    const requestQueue = await Apify.openRequestQueue();
    await requestQueue.addRequest({ url: 'https://apify.com' });

    const handlePageFunction = async ({ request, $ }) => {
        const title = $('title').text();

        console.log(`The title of "${request.url}" is: ${title}.`);
    };

    // Set up the crawler, passing a single options object as an argument.
    const crawler = new Apify.CheerioCrawler({
        requestQueue,
        handlePageFunction,
    });

    await crawler.run();
});
```

--------------------------------

### Install Apify CLI Globally

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

Installs the Apify Command Line Interface globally, enabling project creation and management for Apify SDK.

```Shell
npm install -g apify-cli
```

--------------------------------

### Verify Node.js and NPM Installation

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

Checks the installed versions of Node.js and NPM to ensure prerequisites are met for Apify SDK development.

```Shell
node -v
```

```Shell
npm -v
```

--------------------------------

### Apify API: Getting Started Section

Source: https://docs.apify.com/api/v2/user-get

An introductory section providing guidance and initial steps for developers to begin interacting with the Apify API. It covers fundamental concepts and setup instructions.

```APIDOC
API Section: /api/v2/getting-started
```

--------------------------------

### Create New Apify SDK Project

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

Initializes a new Apify SDK project using the Apify CLI, prompting for a template selection (e.g., 'Hello world'). This command creates a new directory and installs necessary dependencies.

```Shell
apify create my-new-project
```

--------------------------------

### Install Crawlee Meta-Package

Source: https://docs.apify.com/sdk/js/docs/3.2/upgrading/upgrading-to-v3

Install the main 'crawlee' meta-package, which re-exports most of the '@crawlee/*' packages and contains all crawler classes, simplifying installation for general use cases.

```bash
npm install crawlee
```

--------------------------------

### API: Getting Started with Apify API v2

Source: https://docs.apify.com/api/v2/storage-key-value-stores

This section provides an entry point for understanding how to interact with the Apify API v2, with a link to the main getting started guide.

```APIDOC
Documentation Link: /api/v2/getting-started
```

--------------------------------

### Apify API: Getting Started

Source: https://docs.apify.com/api/v2/act-run-metamorph-post

Provides an entry point for understanding and using the Apify API. This section typically covers general API information and initial setup, though specific endpoints are not detailed beyond the introductory path.

```APIDOC
GET /api/v2/getting-started: Introduction to Apify API
```

--------------------------------

### Apify API v2 Overview and Getting Started

Source: https://docs.apify.com/api/v2/webhook-dispatches-get

Provides an introduction to the Apify API version 2, including general information and a link to the getting started guide.

```APIDOC
Apify API v2
  Getting started: /api/v2/getting-started
```

--------------------------------

### Apify API v2: Getting Started

Source: https://docs.apify.com/api/v2/actor-run-abort-post

Provides an entry point for understanding the Apify API v2, covering initial setup and basic concepts for interacting with the platform.

```APIDOC
/api/v2/getting-started
```

--------------------------------

### Apify API: Getting Started Endpoint

Source: https://docs.apify.com/api/v2/act-run-sync-get-dataset-items-get

This snippet provides the base endpoint for getting started with the Apify API v2. It serves as an entry point for understanding the API structure and initial setup.

```APIDOC
GET /api/v2/getting-started
```

--------------------------------

### Example Single Category Store URL

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

An example URL for accessing a specific category (ENTERTAINMENT) within the Apify store. This demonstrates a typical start URL for a focused crawl when a single category is targeted.

```URL
https://apify.com/store?category=ENTERTAINMENT
```

--------------------------------

### Apify API: Getting Started

Source: https://docs.apify.com/api/v2/key-value-stores-get

Overview of the Apify API v2, providing a starting point for interacting with the platform programmatically.

```APIDOC
Getting Started:
  Endpoint: /api/v2/getting-started
```

--------------------------------

### API Reference: Getting Started

Source: https://docs.apify.com/api/v2/actor-build-delete

Provides the introductory endpoint for the Apify API, guiding users on how to begin interacting with the platform's programmatic interface.

```APIDOC
Getting started: /api/v2/getting-started
```

--------------------------------

### Apify API v2 Overview and Getting Started

Source: https://docs.apify.com/api/v2/actors-actor-versions

This section outlines the main entry point for the Apify API v2 and provides a link to the getting started guide. It serves as the top-level navigation for all subsequent API categories.

```APIDOC
Apify API v2:
  GET /api/v2/getting-started - Getting started guide
```

--------------------------------

### Apify API Getting Started

Source: https://docs.apify.com/api/v2/dataset-delete

Provides an overview of the Apify API and its main entry point for initial setup and understanding.

```APIDOC
/api/v2/getting-started: Getting started with Apify API
```

--------------------------------

### Install Crawlee Meta-Package

Source: https://docs.apify.com/sdk/js/reference/changelog

Installs the main Crawlee meta-package from the 'next' distribution tag, which includes most `@crawlee/*` packages and crawler classes. This is a common starting point for new Crawlee projects.

```bash
npm install crawlee@next
```

--------------------------------

### Multiple Start URLs for Apify Store Categories

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

A list of manually collected start URLs for different categories in the Apify store. This approach is used when dynamic scraping of category links is not feasible with tools like CheerioCrawler due to JavaScript dependencies, requiring a pre-defined set of entry points.

```URL
https://apify.com/store?category=TRAVEL
https://apify.com/store?category=ECOMMERCE
https://apify.com/store?category=ENTERTAINMENT
```

--------------------------------

### Apify API Getting Started

Source: https://docs.apify.com/api/v2/post-resurrect-run

Provides an entry point for understanding and interacting with the Apify API.

```APIDOC
GET /api/v2/getting-started - Getting started with Apify API
```

--------------------------------

### Initialize Apify Actor Development in Existing Project

Source: https://docs.apify.com/cli/docs/installation

Explains how to configure an existing project directory for Apify Actor development using `apify init`. This command sets up necessary local files like `.actor/actor.json` and `apify_storage`. It also provides an example of how to define the Actor's start command in the `package.json` file.

```bash
cd ./my/awesome/project
apify init
```

```json
{
    ...
    "scripts": {
        "start": "node your_main_file.js"
    },
    ...
}
```

--------------------------------

### Apify API Getting Started

Source: https://docs.apify.com/api/v2/actor-task-put

Overview of the Apify API, providing a starting point for integration and understanding the API structure.

```APIDOC
GET /api/v2/getting-started: Getting started with Apify API
```

--------------------------------

### Full Apify CheerioCrawler for Category and Detail Pages

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This comprehensive JavaScript example demonstrates a complete Apify CheerioCrawler setup. It initializes the crawler with predefined category URLs, manages request queues, and implements a `handlePageFunction` that conditionally scrapes detailed actor information from detail pages or enqueues new links from category pages, ensuring efficient navigation and data collection.

```JavaScript
const Apify = require('apify');

Apify.main(async () => {
    const sources = [
        'https://apify.com/store?category=TRAVEL',
        'https://apify.com/store?category=ECOMMERCE',
        'https://apify.com/store?category=ENTERTAINMENT',
    ];

    const requestList = await Apify.openRequestList('categories', sources);
    const requestQueue = await Apify.openRequestQueue();

    const crawler = new Apify.CheerioCrawler({
        maxRequestsPerCrawl: 50,
        requestList,
        requestQueue,
        handlePageFunction: async ({ $, request }) => {
            console.log(`Processing ${request.url}`);

            // This is our new scraping logic.
            if (request.userData.detailPage) {
                const urlArr = request.url.split('/').slice(-2);

                const results = {
                    url: request.url,
                    uniqueIdentifier: urlArr.join('/'),
                    owner: urlArr[0],
                    title: $('header h1').text(),
                    description: $('header span.actor-description').text(),
                    modifiedDate: new Date(
                        Number($('ul.ActorHeader-stats time').attr('datetime')),
                    ),
                    runCount: Number(
                        $('ul.ActorHeader-stats > li:nth-of-type(3)')
                            .text()
                            .match(/[\d,]+/)[0]
                            .replace(',', ''),
                    ),
                };
                console.log('RESULTS', results);
            }

            // Only enqueue new links from the category pages.
            if (!request.userData.detailPage) {
                await Apify.utils.enqueueLinks({
                    $,
                    requestQueue,
                    selector: 'div.item > a',
                    baseUrl: request.loadedUrl,
                    transformRequestFunction: (req) => {
                        req.userData.detailPage = true;
                        return req;
                    },
                });
            }
        },
    });

    await crawler.run();
});
```

--------------------------------

### Install Specific Crawlee Package (Cheerio)

Source: https://docs.apify.com/sdk/js/docs/3.2/upgrading/upgrading-to-v3

Install a specific Crawlee package, such as '@crawlee/cheerio', when only a particular functionality is required, minimizing the installed dependencies.

```bash
npm install @crawlee/cheerio
```

--------------------------------

### Run Apify Actor and Retrieve Data

Source: https://docs.apify.com/api/client/python/docs/overview/getting-started

Demonstrates how to programmatically start an Apify Actor, wait for its completion, and then fetch the results from its default dataset using the Apify Client for Python. This example requires an Apify API token and the target Actor's ID.

```Python (Async)
from apify_client import ApifyClientAsync

# You can find your API token at https://console.apify.com/settings/integrations.
TOKEN = 'MY-APIFY-TOKEN'

async def main() -> None:
    apify_client = ApifyClientAsync(TOKEN)

    # Start an Actor and wait for it to finish.
    actor_client = apify_client.actor('john-doe/my-cool-actor')
    call_result = await actor_client.call()

    if call_result is None:
        print('Actor run failed.')
        return

    # Fetch results from the Actor run's default dataset.
    dataset_client = apify_client.dataset(call_result['defaultDatasetId'])
    list_items_result = await dataset_client.list_items()
    print(f'Dataset: {list_items_result}')
```

```Python (Sync)
from apify_client import ApifyClient
```

--------------------------------

### Example Dockerfile for Node.js Apify Actor (JavaScript)

Source: https://docs.apify.com/sdk/js/docs/next/guides/docker-images

This Dockerfile provides a standard setup for a Node.js Apify actor using JavaScript. It copies package.json for dependency installation, then copies the rest of the source code, and sets npm start as the default command.

```Dockerfile
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:16

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY . ./

# Run the image.
CMD npm start --silent
```

--------------------------------

### Apify API Getting Started

Source: https://docs.apify.com/api/v2/actor-build-abort-post

Provides an entry point for understanding how to interact with the Apify API, outlining the initial steps for API usage.

```APIDOC
Getting started: /api/v2/getting-started
```

--------------------------------

### Apify API: Getting Started

Source: https://docs.apify.com/api/v2/actor-task-delete

This section provides the introductory endpoint for the Apify API, guiding users on how to begin interacting with the platform's programmatic interface.

```APIDOC
Getting Started:
  Endpoint: /api/v2/getting-started
```

--------------------------------

### Initializing Apify Main Function and Request Queue

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This code illustrates how to use `Apify.main()` to set up the primary execution context for an Apify actor. It also shows the initialization of a `RequestQueue` and adding an initial URL to it, which serves as the starting point for the crawler.

```javascript
const Apify = require('apify');

// This is how you use the Apify.main() function.
Apify.main(async () => {
    // First we create the request queue instance.
    const requestQueue = await Apify.openRequestQueue();
    // And then we add a request to it.
    await requestQueue.addRequest({ url: 'https://apify.com' });
});
```

--------------------------------

### Apify API v2 Getting Started

Source: https://docs.apify.com/api/v2/users

Provides an entry point for understanding how to interact with the Apify API v2.

```APIDOC
Resource: Getting Started
  Endpoint: /api/v2/getting-started
```

--------------------------------

### Install Crawlee Meta-Package

Source: https://docs.apify.com/sdk/js/reference/3.0/changelog

Installs the main Crawlee meta-package from the 'next' distribution tag, providing access to most `@crawlee/*` packages and crawler classes. This is a general installation for comprehensive Crawlee usage.

```Shell
npm install crawlee@next
```

--------------------------------

### Install Apify SDK v1 with Puppeteer

Source: https://docs.apify.com/sdk/js/docs/3.0/upgrading/upgrading-to-v1

This command installs the Apify SDK version 1 along with the Puppeteer browser automation library. This setup is similar to previous SDK versions and is suitable for projects that continue to use Puppeteer.

```bash
npm install apify puppeteer
```

--------------------------------

### Apify API v2 Getting Started

Source: https://docs.apify.com/api/v2/request-queues-post

Provides an entry point for understanding the Apify API v2 structure and how to begin interacting with it.

```APIDOC
GET /api/v2/getting-started
```

--------------------------------

### Apify API v2: Getting Started Endpoint

Source: https://docs.apify.com/api/v2/datasets-post

Provides an overview and initial steps for interacting with the Apify API v2.

```APIDOC
GET /api/v2/getting-started: Getting started with Apify API v2
```

--------------------------------

### Verify Node.js Scraping Setup in main.js

Source: https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup

This JavaScript code snippet, intended for 'main.js', imports the 'gotScraping' and 'cheerio' libraries and prints "it works!" to the console. It serves as a basic test to ensure that the libraries are correctly installed and the ES module setup is functional.

```JavaScript
import { gotScraping } from 'got-scraping';
import * as cheerio from 'cheerio';

console.log('it works!');
```

--------------------------------

### Dockerfile Example for Node.js JavaScript Actor

Source: https://docs.apify.com/sdk/js/docs/3.0/guides/docker-images

This Dockerfile provides a standard setup for an Apify actor written in Node.js with JavaScript. It optimizes build times by copying package.json first for dependency installation, then copying the rest of the source code, and finally defines the command to start the actor.

```Dockerfile
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:16

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY . ./

# Run the image.
CMD npm start --silent
```

--------------------------------

### Install Crawlee with Browser Dependencies (Playwright)

Source: https://docs.apify.com/sdk/js/docs/3.2/upgrading/upgrading-to-v3

Demonstrates how to install Crawlee alongside specific browser automation libraries like Playwright. This approach allows users to control the version of the browser dependency explicitly.

```bash
npm install crawlee playwright
# or npm install @crawlee/playwright playwright
```

--------------------------------

### Install Apify SDK v1 with Puppeteer

Source: https://docs.apify.com/sdk/js/docs/next/upgrading/upgrading-to-v1

This command installs the Apify SDK v1 along with the Puppeteer browser automation library. This setup is similar to previous versions of the SDK where Puppeteer was bundled.

```Shell
npm install apify puppeteer
```

--------------------------------

### Apify API v2 Getting Started

Source: https://docs.apify.com/api/v2/request-queue-head-lock-post

Provides an introduction and initial steps for interacting with the Apify API v2.

```APIDOC
Getting started:
  - Introduction: /api/v2/getting-started
```

--------------------------------

### Install Apify CLI for Local Development

Source: https://docs.apify.com/platform/actors/development/quick-start/web-ide

Instructions for installing the Apify Command Line Interface (CLI) tool, which is essential for interacting with the Apify platform from your local machine. Provides commands for both macOS/Linux (using Homebrew) and other platforms (using npm).

```bash
brew install apify-cli
```

```npm
npm -g install apify-cli
```

--------------------------------

### Example Dockerfile for Node.js JavaScript Actor

Source: https://docs.apify.com/sdk/js/docs/3.2/guides/docker-images

This Dockerfile provides a standard setup for an Apify Node.js actor written in JavaScript. It demonstrates how to efficiently copy `package.json` for layer caching, install production dependencies, and then copy the rest of the source code before defining the `npm start` command. This ensures a fast build process for iterative development.

```Dockerfile
# Specify the base Docker image. You can read more about
# the available images at https://crawlee.dev/docs/guides/docker-images
# You can also use any other image from Docker Hub.
FROM apify/actor-node:16

# Copy just package.json and package-lock.json
# to speed up the build using Docker layer cache.
COPY package*.json ./

# Install NPM packages, skip optional and development dependencies to
# keep the image small. Avoid logging too much and print the dependency
# tree for debugging
RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

# Next, copy the remaining files and directories with the source code.
# Since we do this after NPM install, quick build will be really fast
# for most source file changes.
COPY . ./

# Run the image.
CMD npm start --silent

```

--------------------------------

### Example Output of Apify CLI Version Check

Source: https://docs.apify.com/cli/docs/next/installation

Illustrates the expected output when verifying the Apify CLI installation, showing the CLI version, operating system, and Node.js version.

```text
apify-cli/0.19.1 linux-x64 node-v18.17.0
```

--------------------------------

### Full Apify CheerioCrawler Example with Manual Link Enqueuing

Source: https://docs.apify.com/sdk/js/docs/2.3/guides/getting-started

A complete Apify `CheerioCrawler` example demonstrating how to initialize a request queue, add a starting URL, handle page content (extracting titles and links), manually filter same-domain links, and enqueue them back into the queue. This example also integrates the `maxRequestsPerCrawl` limit.

```JavaScript
const { URL } = require('url');
const Apify = require('apify');

Apify.main(async () => {
    const requestQueue = await Apify.openRequestQueue();
    await requestQueue.addRequest({ url: 'https://apify.com' });

    const handlePageFunction = async ({ request, $ }) => {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Here starts the new part of handlePageFunction.
        const links = $('a[href]')
            .map((i, el) => $(el).attr('href'))
            .get();

        const { origin } = new URL(request.loadedUrl);
        const absoluteUrls = links.map(
            (link) => new URL(link, request.loadedUrl),
        );

        const sameDomainLinks = absoluteUrls.filter(
            (url) => url.origin === origin,
        );

        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
        for (const url of sameDomainLinks) {
            await requestQueue.addRequest({ url: url.href });
        }
    };

    const crawler = new Apify.CheerioCrawler({
        maxRequestsPerCrawl: 20,
        requestQueue,
        handlePageFunction,
    });

    await crawler.run();
});
```

--------------------------------

### Apify API v2: Getting Started and General Actor Management Endpoints

Source: https://docs.apify.com/api/v2/act-runs-get

This section details the initial setup and core API endpoints for managing Actors on the Apify platform. It covers operations such as listing, creating, retrieving, updating, and deleting Actors.

```APIDOC
GET /api/v2/getting-started - Introduction to Apify API v2
GET /api/v2/actors - Get list of Actors
POST /api/v2/acts-post - Create Actor
GET /api/v2/act-get - Get Actor
PUT /api/v2/act-put - Update Actor
DELETE /api/v2/act-delete - Delete Actor
```

--------------------------------

### Apify API Getting Started

Source: https://docs.apify.com/api/v2/schedule-delete

An introductory overview to the Apify API, providing a starting point for understanding its structure and capabilities.

```APIDOC
GET /api/v2/getting-started - Getting started with Apify API
```

--------------------------------

### Apify SDK CheerioCrawler Example with Manual Same-Domain Link Enqueueing

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This JavaScript code demonstrates a basic web crawler using Apify's `CheerioCrawler`. It initializes a `RequestQueue`, adds a starting URL, and defines a `handlePageFunction` to extract page titles. Crucially, it manually parses all `<a>` tags, filters for same-domain links, converts them to absolute URLs, and enqueues them for further crawling. The crawler is configured with `maxRequestsPerCrawl`.

```JavaScript
const { URL } = require('url'); // <------ This is new.
const Apify = require('apify');

Apify.main(async () => {
    const requestQueue = await Apify.openRequestQueue();
    await requestQueue.addRequest({ url: 'https://apify.com' });

    const handlePageFunction = async ({ request, $ }) => {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Here starts the new part of handlePageFunction.
        const links = $('a[href]')
            .map((i, el) => $(el).attr('href'))
            .get();

        const ourDomain = 'https://apify.com';
        const absoluteUrls = links.map((link) => new URL(link, ourDomain));

        const sameDomainLinks = absoluteUrls.filter((url) =>
            url.href.startsWith(ourDomain),
        );

        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
        for (const url of sameDomainLinks) {
            await requestQueue.addRequest({ url: url.href });
        }
    };

    const crawler = new Apify.CheerioCrawler({
        maxRequestsPerCrawl: 20, // <------ This is new too.
        requestQueue,
        handlePageFunction,
    });

    await crawler.run();
});
```

--------------------------------

### Quickstart: Configure Apify Proxy using SDKs

Source: https://docs.apify.com/platform/proxy

These examples demonstrate how to quickly set up and use Apify Proxy in your applications. The JavaScript example uses `PuppeteerCrawler` from Crawlee, while the Python example integrates with the `requests` library. Both show how to create a proxy configuration and make an anonymous request to test the proxy.

```JavaScript
import { Actor } from 'apify';
import { PuppeteerCrawler } from 'crawlee';

await Actor.init();

const proxyConfiguration = await Actor.createProxyConfiguration();

const crawler = new PuppeteerCrawler({
    proxyConfiguration,
    async requestHandler({ page }) {
        console.log(await page.content());
    },
});

await crawler.run(['https://proxy.apify.com/?format=json']);

await Actor.exit();
```

```Python
import requests, asyncio
from apify import Actor

async def main():
    async with Actor:
        proxy_configuration = await Actor.create_proxy_configuration()
        proxy_url = await proxy_configuration.new_url()

        proxies = {
            'http': proxy_url,
            'https': proxy_url,
        }

        response = requests.get('https://api.apify.com/v2/browser-info', proxies=proxies)
        print(response.text)

if __name__ == '__main__':
    asyncio.run(main())
```

--------------------------------

### Run Apify SDK Project Locally with Purge

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

Navigates into the newly created project directory and executes the Apify actor. The '-p' flag stands for '--purge' and clears out persistent storages before starting the actor, preventing old state from interfering with the current run.

```Shell
cd my-new-project
```

```Shell
apify run -p
```

--------------------------------

### Install Apify Command-Line Interface (CLI)

Source: https://docs.apify.com/sdk/js/docs/2.3/guides/quick-start

Installs the Apify CLI globally, which helps manage Apify projects, create boilerplates, and deploy code to the Apify platform.

```Shell
npm -g install apify-cli
```

--------------------------------

### Apify API v2 Getting Started

Source: https://docs.apify.com/api/v2

Provides the foundational entry point for interacting with the Apify API v2, indicating the base path for all subsequent API calls.

```APIDOC
API Endpoint: /api/v2/getting-started
```

--------------------------------

### Apify API Getting Started Endpoint

Source: https://docs.apify.com/api/v2/dataset-put

This section outlines the initial endpoint for interacting with the Apify API, providing a starting point for API integration.

```APIDOC
GET /api/v2/getting-started - Get started with the Apify API
```

--------------------------------

### Configure package.json Start Script for Apify Actor

Source: https://docs.apify.com/cli/docs/next/installation

Modifies the `package.json` file to define the start command for an Apify Actor, allowing it to be run locally using `apify run`. The `start` script should point to the Actor's main file.

```json
{
    ...
    "scripts": {
        "start": "node your_main_file.js"
    },
    ...
}
```

--------------------------------

### Quick Start: Run Actor and Fetch Dataset Items

Source: https://docs.apify.com/api/client/js/docs/2

This example demonstrates a common quick start scenario: programmatically starting an Apify Actor and then retrieving the results from its default dataset after the Actor has completed its execution. It showcases basic interaction with the Apify API using the client library.

```JavaScript
import { ApifyClient } from 'apify-client';

const client = new ApifyClient({ token: 'MY-APIFY-TOKEN' });

// Starts an Actor and waits for it to finish
const { defaultDatasetId } = await client.actor('username/actor-name').call();

// Lists items from the Actor's dataset
const { items } = await client.dataset(defaultDatasetId).listItems();
```

--------------------------------

### Example Apify Actor Input JSON for Categories

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This JSON array provides an example of the expected structure for the actor's input, specifically for defining categories to be scraped. This input can be set via the Apify platform's actor input form or by creating an 'INPUT.json' file locally.

```JSON
["TRAVEL", "ECOMMERCE", "ENTERTAINMENT"]
```

--------------------------------

### Install Crawlee Meta-Package

Source: https://docs.apify.com/sdk/js/reference/3.3/changelog

Installs the main Crawlee meta-package from the 'next' distribution tag. This package re-exports most @crawlee/* packages, providing all core crawler classes and functionalities.

```bash
npm install crawlee@next
```

--------------------------------

### Create New Apify Project

Source: https://docs.apify.com/sdk/js/docs/2.3/guides/getting-started

Initializes a new Apify SDK project with a specified name. This command prompts the user to select a project template, creates a new directory, sets up package.json, and installs necessary dependencies.

```bash
apify create my-new-project
```

--------------------------------

### Simple Dockerfile for Apify Node.js Actor

Source: https://docs.apify.com/sdk/js/docs/3.0/guides/docker-images

This Dockerfile provides a basic setup for an Apify Node.js actor. It copies package files, installs dependencies, copies source code, and defines the start command. It's optimized for quick rebuilds by leveraging Docker layer caching.

```Dockerfile
COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent
```

--------------------------------

### Install Apify SDK v1 with Puppeteer

Source: https://docs.apify.com/sdk/js/docs/2.3/guides/migration-to-v1

Installs Apify SDK v1 along with the Puppeteer browser automation library. This setup is suitable for projects that rely on Puppeteer for web scraping and automation tasks, maintaining compatibility with previous SDK versions' Puppeteer-based functionalities.

```javascript
npm install apify puppeteer
```

--------------------------------

### Basic Apify enqueueLinks usage

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

Demonstrates a standard usage of Apify.utils.enqueueLinks() from a previous chapter, showing how to enqueue links based on pseudo URLs.

```javascript
await enqueueLinks({
    $,
    requestQueue,
    pseudoUrls: ['http[s?]://apify.com[.*]'],
    baseUrl: request.loadedUrl,
});
```

--------------------------------

### Initialize Crawlee Project with CLI

Source: https://docs.apify.com/academy/web-scraping-for-beginners/challenge/initializing-and-setting-up

This command uses the Crawlee CLI to create a new project directory with a specified name. It prompts the user to select a boilerplate template, such as CheerioCrawler, for the project's initial setup.

```shell
npx crawlee create amazon-crawler
```

--------------------------------

### Basic Dockerfile for Apify Node.js Actor

Source: https://docs.apify.com/sdk/js/docs/3.1/guides/docker-images

This Dockerfile provides a basic setup for an Apify Node.js actor. It copies package files, installs production dependencies, and then copies the application source code. The `npm start` command is used to run the actor.

```Dockerfile
COPY --chown=myuser package*.json ./

RUN npm --quiet set progress=false \
    && npm install --omit=dev --omit=optional \
    && echo "Installed NPM packages:" \
    && (npm list --omit=dev --all || true) \
    && echo "Node.js version:" \
    && node --version \
    && echo "NPM version:" \
    && npm --version

COPY --chown=myuser . ./

CMD npm start --silent
```

--------------------------------

### Install Apify Python SDK

Source: https://docs.apify.com/platform/integrations/qdrant

This command installs the necessary Apify Python SDK package using pip, enabling programmatic interaction with Apify services.

```Python
pip install apify-client
```

--------------------------------

### Install Crawlee Cheerio Package Only

Source: https://docs.apify.com/sdk/js/reference/3.0/changelog

Installs only the `@crawlee/cheerio` package from the 'next' distribution tag. This is suitable for projects that specifically require Cheerio support and want to minimize dependencies.

```Shell
npm install @crawlee/cheerio@next
```

--------------------------------

### Apify CheerioCrawler: Automated Link Enqueuing with enqueueLinks()

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This JavaScript example showcases an Apify CheerioCrawler utilizing the `Apify.utils.enqueueLinks()` helper function. It replaces manual link processing with an automated solution, simplifying the `handlePageFunction` by automatically extracting and enqueuing links based on provided pseudo-URLs.

```JavaScript
const Apify = require('apify');
const {
    utils: { enqueueLinks },
} = Apify;

Apify.main(async () => {
    const requestQueue = await Apify.openRequestQueue();
    await requestQueue.addRequest({ url: 'https://apify.com' });

    const handlePageFunction = async ({ request, $ }) => {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Enqueue links
        const enqueued = await enqueueLinks({
            $,
            requestQueue,
            pseudoUrls: ['http[s?]://apify.com[.*]'],
            baseUrl: request.loadedUrl,
        });
        console.log(`Enqueued ${enqueued.length} URLs.`);
    };

    const crawler = new Apify.CheerioCrawler({
        maxRequestsPerCrawl: 20,
        requestQueue,
        handlePageFunction,
    });

    await crawler.run();
});
```

--------------------------------

### Configure TypeScript for Crawlee/Apify Projects

Source: https://docs.apify.com/sdk/js/docs/3.2/upgrading/upgrading-to-v3

Provides a 'tsconfig.json' example for Crawlee and Apify SDK projects. It extends '@apify/tsconfig' and sets 'module' and 'target' to 'ES2022' or above to enable features like top-level await.

```json
{
    "extends": "@apify/tsconfig",
    "compilerOptions": {
        "module": "ES2022",
        "target": "ES2022",
        "outDir": "dist",
        "lib": ["DOM"]
    },
    "include": ["./src/**/*"]
}
```

--------------------------------

### Initialize a new npm project

Source: https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup

This command initializes a new npm project in the current directory, creating a package.json file. The -y flag automatically answers 'yes' to all prompts, allowing for quick default project setup without interactive questions.

```Bash
npm init -y
```

--------------------------------

### Complete Example: Save Data and Use OpenAI Assistant

Source: https://docs.apify.com/platform/integrations/openai-assistants

This comprehensive snippet combines the initialization, assistant creation, and vector store setup steps into a single block, providing a partial, runnable example for integrating Apify data with an OpenAI Assistant.

```Python
from apify_client import ApifyClient
from openai import OpenAI

client = OpenAI(api_key="YOUR-OPENAI-API-KEY")
apify_client = ApifyClient("YOUR-APIFY-API-TOKEN")

my_assistant = client.beta.assistants.create(
    instructions="As a customer support agent at Apify, your role is to assist customers",
    name="Support assistant",
    tools=[{"type": "file_search"}],
    model="gpt-4o-mini"
)

# Create a vector store
vector_store = client.beta.vector_stores.create(name="Support assistant vector store")
```

--------------------------------

### Apify CheerioCrawler Main Entry Point (main.js)

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/getting-started

This `main.js` file sets up the core Apify CheerioCrawler. It initializes the request list and queue, integrates helper functions for source retrieval and request routing, and orchestrates the crawling process. This central file defines the overall flow of the actor.

```JavaScript
// main.js
const Apify = require('apify');
const tools = require('./tools');
const {
    utils: { log },
} = Apify;

Apify.main(async () => {
    log.info('Starting actor.');
    const requestList = await Apify.openRequestList(
        'categories',
        await tools.getSources(),
    );
    const requestQueue = await Apify.openRequestQueue();
    const router = tools.createRouter({ requestQueue });

    log.debug('Setting up crawler.');
    const crawler = new Apify.CheerioCrawler({
        requestList,
        requestQueue,
        handlePageFunction: async (context) => {
            const { request } = context;
            log.info(`Processing ${request.url}`);
            await router(request.userData.label, context);
        },
    });

    log.info('Starting the crawl.');
    await crawler.run();
    log.info('Actor finished.');
});
```

--------------------------------

### Example .actor/actor.json Configuration File

Source: https://docs.apify.com/cli/docs/installation

This JSON snippet provides a concrete example of the `.actor/actor.json` file. It demonstrates how to define an Actor's name, version, build tag, environment variables, and paths to Dockerfile, README, and input/storage schemas, associating a local project with an Apify Actor.

```JSON
{
  "actorSpecification": 1,
  "name": "name-of-my-scraper",
  "version": "0.0",
  "buildTag": "latest",
  "environmentVariables": {
    "MYSQL_USER": "my_username",
    "MYSQL_PASSWORD": "@mySecretPassword"
  },
  "dockerfile": "./Dockerfile",
  "readme": "./ACTOR.md",
  "input": "./input_schema.json",
  "storages": {
    "dataset": "./dataset_schema.json"
  }
}
```

--------------------------------

### Apify API Getting Started Endpoint

Source: https://docs.apify.com/api/v2/request-queue-delete

This entry point provides general information and initial steps for interacting with the Apify API. It serves as a foundational reference for new API users.

```APIDOC
GET /api/v2/getting-started - Getting started with Apify API
```

--------------------------------

### Quickstart: Configure and Use Apify Proxy with SDKs

Source: https://docs.apify.com/proxy

This section provides quickstart examples for integrating Apify Proxy into your scraping jobs using both JavaScript and Python SDKs. It demonstrates how to initialize the Actor, create a proxy configuration, and make requests through the Apify Proxy to ensure anonymous access and prevent IP-based blocking.

```JavaScript
import { Actor } from 'apify';
import { PuppeteerCrawler } from 'crawlee';

await Actor.init();

const proxyConfiguration = await Actor.createProxyConfiguration();

const crawler = new PuppeteerCrawler({
    proxyConfiguration,
    async requestHandler({ page }) {
        console.log(await page.content());
    },
});

await crawler.run(['https://proxy.apify.com/?format=json']);

await Actor.exit();
```

```Python
import requests, asyncio
from apify import Actor

async def main():
    async with Actor:
        proxy_configuration = await Actor.create_proxy_configuration()
        proxy_url = await proxy_configuration.new_url()

        proxies = {
            'http': proxy_url,
            'https': proxy_url,
        }

        response = requests.get('https://api.apify.com/v2/browser-info', proxies=proxies)
        print(response.text)

if __name__ == '__main__':
    asyncio.run(main())
```

--------------------------------

### Install Apify Python SDK

Source: https://docs.apify.com/platform/integrations/milvus

Installs the necessary Apify Python SDK using pip, enabling programmatic interaction with Apify actors and datasets.

```Python
pip install apify-client
```

--------------------------------

### Initialize Apify Client with API Token (Python)

Source: https://docs.apify.com/api/client/python/docs/overview/setting-up

Demonstrates how to initialize the Apify client using an API token. This snippet provides examples for both asynchronous (ApifyClientAsync) and synchronous (ApifyClient) client instantiation, showing the basic setup required to interact with the Apify API.

```Python
from apify_client import ApifyClientAsync

TOKEN = 'MY-APIFY-TOKEN'

async def main() -> None:
    # Client initialization with the API token.
    apify_client = ApifyClientAsync(TOKEN)

```

```Python
from apify_client import ApifyClient

TOKEN = 'MY-APIFY-TOKEN'

def main() -> None:
    # Client initialization with the API token.
    apify_client = ApifyClient(TOKEN)

```

--------------------------------

### Install Apify SDK and Playwright

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/quick-start

This command installs the Apify SDK and Playwright packages into your Node.js project. Playwright is not bundled with the SDK and needs to be installed separately to enable browser automation.

```Shell
npm install apify playwright
```

--------------------------------

### Run Apify Actor with Actor.main Wrapper

Source: https://docs.apify.com/sdk/js/docs/3.2/upgrading/upgrading-to-v3

Shows how to use `Actor.main()` as a convenience wrapper for `Actor.init()` and `Actor.exit()`. This method handles the setup, teardown, and error wrapping for the main actor logic, simplifying the execution flow.

```JavaScript
import { Actor } from 'apify';

await Actor.main(
    async () => {
        // your code
    },
    { statusMessage: 'Crawling finished!' },
);
```

--------------------------------

### Install Apify CLI Globally

Source: https://docs.apify.com/sdk/js/docs/1.3/guides/quick-start

This command installs the Apify command-line interface (CLI) tool globally on your system. The CLI simplifies project creation, local execution, and deployment to the Apify platform.

```Shell
npm -g install apify-cli
```

--------------------------------

### Apify API: Getting Started

Source: https://docs.apify.com/api/v2/actor-run-delete

This section provides the entry point for understanding the Apify API v2.

```APIDOC
GET /api/v2/getting-started - Getting started with Apify API v2
```

--------------------------------

### Complete Apify CheerioCrawler for Web Scraping and Data Saving

Source: https://docs.apify.com/sdk/js/docs/2.3/guides/getting-started

A comprehensive example of an Apify CheerioCrawler setup. It includes initializing request lists, managing request queues, handling both category and detail pages, and saving extracted data using `Apify.pushData()`.

```JavaScript
const Apify = require('apify');

Apify.main(async () => {
    const sources = [
        'https://apify.com/store?category=TRAVEL',
        'https://apify.com/store?category=ECOMMERCE',
        'https://apify.com/store?category=ENTERTAINMENT',
    ];

    const requestList = await Apify.openRequestList('categories', sources);
    const requestQueue = await Apify.openRequestQueue();

    const crawler = new Apify.CheerioCrawler({
        maxRequestsPerCrawl: 50,
        requestList,
        requestQueue,
        handlePageFunction: async ({ $, request }) => {
            console.log(`Processing ${request.url}`);

            // This is our new scraping logic.
            if (request.userData.detailPage) {
                const urlArr = request.url.split('/').slice(-2);

                const results = {
                    url: request.url,
                    uniqueIdentifier: urlArr.join('/'),
                    owner: urlArr[0],
                    title: $('header h1').text(),
                    description: $('header span.actor-description').text(),
                    modifiedDate: new Date(
                        Number($('ul.ActorHeader-stats time').attr('datetime')),
                    ),
                    runCount: Number(
                        $('ul.ActorHeader-stats > li:nth-of-type(3)')
                            .text()
                            .match(/[\d,]+/)[0]
                            .replace(',', ''),
                    ),
                };
                await Apify.pushData(results);
            }

            // Only enqueue new links from the category pages.
            if (!request.userData.detailPage) {
                await Apify.utils.enqueueLinks({
                    $,
                    requestQueue,
                    selector: 'div.item > a',
                    baseUrl: request.loadedUrl,
                    transformRequestFunction: (req) => {
                        req.userData.detailPage = true;
                        return req;
                    },
                });
            }
        },
    });

    await crawler.run();
});
```